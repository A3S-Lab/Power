# A3S Power â€” Next Phase Development Plan

## First Principles Analysis

### Core Mission
A3S Power exists to solve one problem: **LLM inference where the infrastructure operator cannot see your data**. Hardware TEE (SEV-SNP / TDX) enforces memory encryption; Power provides the cryptographic proof chain that lets clients verify it.

### What Makes Power Unique (The Moat)
1. **Hardware-enforced privacy** â€” not policy promises, but CPU-level memory encryption
2. **Verifiable inference** â€” client can cryptographically prove which model ran, unmodified
3. **tee-minimal build** â€” ~1,220 deps, zero C, fully auditable supply chain
4. **Layer-streaming** â€” O(layer_size) peak RAM, runs 7B+ models in 512MB EPC

### Current State (v0.3.0)
- 3 backends (mistralrs, llamacpp, picolm) â€” all functional
- Full TEE stack (attestation, encrypted models, RA-TLS, privacy, audit)
- OpenAI-compatible API with streaming
- picolm: 14+ tok/s decode, pure Rust, layer-streaming, 868 tests

### First Principles Question: What Should We Build Next?

Every candidate feature must pass this filter:
1. Does it directly strengthen the privacy/verifiability moat?
2. Does it make Power deployable in real TEE production environments?
3. Does it close a gap that blocks actual adoption?

Features that fail this filter get rejected, no matter how "nice to have" they are.

---

## Phase 4: Production TEE Readiness âœ…

**Goal**: Make Power deployable in real AMD SEV-SNP / Intel TDX environments with confidence.

### 4.1 â€” âœ… picolm Multi-Turn Session KV Cache
- `Arc<Mutex<Option<KvCache>>>` return path via `tokio::spawn` background task
- KV cache positions correctly maintained across turns
- Session map insert/remove with eviction on unload

### 4.2 â€” âœ… picolm Chat Template from GGUF Metadata
- Read `tokenizer.chat_template` from GGUF metadata
- Jinja2 rendering via minijinja with ChatML fallback
- Tested with Llama 3 and invalid template fallback

### 4.3 â€” âœ… picolm Configurable Context Length
- Read `context_length` from GGUF metadata, capped at 32K
- KV cache allocation scaled accordingly

### 4.4 â€” âœ… picolm Stop Sequence Support
- Check generated text against `stop` sequences after each token
- Trim output at stop boundary, set `finish_reason: "stop"`

### 4.5 â€” âœ… Remove Unused `candle-core` from picolm Feature
- Removed from Cargo.toml, picolm feature now: `["dep:memmap2", "dep:half", "dep:rayon"]`

### 4.6 â€” âœ… Integration Tests
- `tests/integration.rs`: 14 tests (HTTP API, router, registry, auth, error paths)
- `tests/picolm_tee.rs`: 8 tests (load/unload cycle, TEE mode, deterministic output)
- `tests/picolm_real.rs`: Real model inference (gated by model file presence)

---

## Phase 5: Performance & Scalability â€” Partially Complete

**Goal**: Close the remaining performance gaps vs. llama.cpp for TEE-constrained environments.

### 5.1 â€” âœ… SIMD-Accelerated vec_dot Kernels
- AVX2+FMA kernels for F32, Q8_0, Q4_K, Q6_K
- Runtime feature detection via `is_x86_feature_detected!`
- Scalar fallback for non-AVX2 platforms (aarch64)
- Parity tests: AVX2 vs scalar for all kernel types

### 5.2 â€” ğŸš§ NEON-Accelerated vec_dot for Apple Silicon (Priority: LOW)
- `#[cfg(target_arch = "aarch64")]` NEON paths
- Dev convenience only, not TEE-relevant

### 5.3 â€” ğŸš§ Batch Prefill (Priority: MEDIUM)
- Batch Q/K/V projections: matmul instead of matvec for prefill
- Only for prefill phase; decode stays single-token

### 5.4 â€” ğŸš§ Speculative Decoding (Priority: LOW)
- Evaluate after 5.1-5.3

---

## Phase 6: TEE Hardening âœ…

**Goal**: Close security gaps for production TEE deployment.

### 6.1 â€” âœ… Timing Side-Channel Mitigation
- `timing_padding_ms` wired into both streaming and non-streaming chat paths
- Â±20% jitter via existing `timing_padding()` method

### 6.2 â€” âœ… Memory Zeroization Audit
- `Drop` impl for `ForwardBuffers`: zeroizes all 12 Vec<f32> buffers
- `Drop` impl for `LayerKvCache`: zeroizes K/V f16 data
- `KvCache::clear()` now zeroizes data instead of just resetting length

### 6.3 â€” âœ… Startup Self-Test
- Embedded test vectors for rms_norm, vec_dot_f32, vec_dot_q8_0
- Runs at model load time, fails fast with clear error on mismatch
- Catches memory corruption in TEE before inference begins

---

## Phase 7: Ecosystem Integration â€” Partially Complete

**Goal**: Make Power useful in the broader A3S platform.

### 7.1 â€” ğŸš§ picolm Tool/Function Calling (Priority: MEDIUM)
- Wire `tool_parser.rs` into picolm response stream
- Parse tool call JSON from model output
- Return structured `tool_calls` in response chunks

### 7.2 â€” ğŸš§ picolm Structured Output (JSON Schema) (Priority: MEDIUM)
- Implement grammar-constrained sampling in picolm
- Wire `json_schema.rs` grammar into the token sampling loop

### 7.3 â€” âœ… picolm Repeat/Frequency Penalty
- 64-token ring buffer tracking recent generated tokens
- `repeat_penalty` (multiplicative, llama.cpp style)
- `frequency_penalty` (proportional to count, OpenAI style)
- `presence_penalty` (flat if appeared, OpenAI style)
- Applied to logits before sampling

---

## What We Will NOT Build (First Principles Rejection)

### âŒ GPU Support for picolm
**Why not**: TEE memory (EPC) is CPU memory. GPU memory is outside the trust boundary. Adding GPU to picolm would undermine the entire security model. Use mistralrs/llamacpp for GPU inference outside TEE.

### âŒ Embeddings in picolm
**Why not**: Embedding models are small, don't need layer-streaming, and don't process sensitive user prompts (they process documents at indexing time, not query time). Use mistralrs for embeddings. Adding embedding support to picolm adds complexity without strengthening the privacy moat.

### âŒ Vision/Multimodal in picolm
**Why not**: Vision models require image encoders (ViT) that are architecturally different from text transformers. The complexity cost is high, and the TEE use case for vision is niche. Use mistralrs for vision.

### âŒ Model Quantization in picolm
**Why not**: Quantization is a one-time offline operation. It doesn't need to run inside TEE. Users quantize models before deployment.

### âŒ LoRA/Adapter Support in picolm
**Why not**: LoRA adds complexity to the forward pass and the supply-chain audit story. In TEE, you want to verify one specific model â€” not a base + N adapters. If needed, merge LoRA into the base model before deployment.

---

## Completion Summary

```
Phase 4 (Production TEE Readiness):          âœ… ALL COMPLETE
  4.5 Remove candle-core from picolm         âœ…
  4.1 Multi-turn KV cache                    âœ…
  4.2 Chat template from GGUF                âœ…
  4.4 Stop sequence support                  âœ…
  4.3 Configurable context length            âœ…
  4.6 Integration tests                      âœ…

Phase 6 (TEE Hardening):                     âœ… ALL COMPLETE
  6.1 Timing side-channel mitigation         âœ…
  6.2 Memory zeroization audit               âœ…
  6.3 Startup self-test                      âœ…

Phase 5 (Performance):                       ğŸš§ PARTIAL
  5.1 AVX2/AVX-512 vec_dot                   âœ… (F32, Q8_0, Q4_K, Q6_K)
  5.3 Batch prefill                          ğŸš§ (not started)
  5.2 NEON vec_dot                           ğŸš§ (not started, low priority)
  5.4 Speculative decoding                   ğŸš§ (not started, low priority)

Phase 7 (Ecosystem):                         ğŸš§ PARTIAL
  7.3 Repeat penalty                         âœ…
  7.1 Tool/function calling                  ğŸš§ (not started)
  7.2 Structured output                      ğŸš§ (not started)
```

**868 tests total** (unit + integration + real model).
**Recommended next**: 5.3 Batch prefill â†’ 7.1 Tool calling â†’ 7.2 Structured output.
