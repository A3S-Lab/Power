# A3S Power ‚Äî Next Phase Development Plan

## First Principles Analysis

### Core Mission
A3S Power exists to solve one problem: **LLM inference where the infrastructure operator cannot see your data**. Hardware TEE (SEV-SNP / TDX) enforces memory encryption; Power provides the cryptographic proof chain that lets clients verify it.

### What Makes Power Unique (The Moat)
1. **Hardware-enforced privacy** ‚Äî not policy promises, but CPU-level memory encryption
2. **Verifiable inference** ‚Äî client can cryptographically prove which model ran, unmodified
3. **tee-minimal build** ‚Äî ~1,220 deps, zero C, fully auditable supply chain
4. **Layer-streaming** ‚Äî O(layer_size) peak RAM, runs 7B+ models in 512MB EPC

### Current State (v0.3.0)
- 3 backends (mistralrs, llamacpp, picolm) ‚Äî all functional
- Full TEE stack (attestation, encrypted models, RA-TLS, privacy, audit)
- OpenAI-compatible API with streaming
- picolm: 14+ tok/s decode, pure Rust, layer-streaming, 868 tests

### First Principles Question: What Should We Build Next?

Every candidate feature must pass this filter:
1. Does it directly strengthen the privacy/verifiability moat?
2. Does it make Power deployable in real TEE production environments?
3. Does it close a gap that blocks actual adoption?

Features that fail this filter get rejected, no matter how "nice to have" they are.

---

## Phase 4: Production TEE Readiness ‚úÖ

**Goal**: Make Power deployable in real AMD SEV-SNP / Intel TDX environments with confidence.

### 4.1 ‚Äî ‚úÖ picolm Multi-Turn Session KV Cache
- `Arc<Mutex<Option<KvCache>>>` return path via `tokio::spawn` background task
- KV cache positions correctly maintained across turns
- Session map insert/remove with eviction on unload

### 4.2 ‚Äî ‚úÖ picolm Chat Template from GGUF Metadata
- Read `tokenizer.chat_template` from GGUF metadata
- Jinja2 rendering via minijinja with ChatML fallback
- Tested with Llama 3 and invalid template fallback

### 4.3 ‚Äî ‚úÖ picolm Configurable Context Length
- Read `context_length` from GGUF metadata, capped at 32K
- KV cache allocation scaled accordingly

### 4.4 ‚Äî ‚úÖ picolm Stop Sequence Support
- Check generated text against `stop` sequences after each token
- Trim output at stop boundary, set `finish_reason: "stop"`

### 4.5 ‚Äî ‚úÖ Remove Unused `candle-core` from picolm Feature
- Removed from Cargo.toml, picolm feature now: `["dep:memmap2", "dep:half", "dep:rayon"]`

### 4.6 ‚Äî ‚úÖ Integration Tests
- `tests/integration.rs`: 14 tests (HTTP API, router, registry, auth, error paths)
- `tests/picolm_tee.rs`: 8 tests (load/unload cycle, TEE mode, deterministic output)
- `tests/picolm_real.rs`: Real model inference (gated by model file presence)

---

## Phase 5: Performance & Scalability ‚Äî Partially Complete

**Goal**: Close the remaining performance gaps vs. llama.cpp for TEE-constrained environments.

### 5.1 ‚Äî ‚úÖ SIMD-Accelerated vec_dot Kernels
- AVX2+FMA kernels for F32, Q8_0, Q4_K, Q6_K
- Runtime feature detection via `is_x86_feature_detected!`
- Scalar fallback for non-AVX2 platforms (aarch64)
- Parity tests: AVX2 vs scalar for all kernel types

### 5.2 ‚Äî ‚úÖ NEON-Accelerated vec_dot for Apple Silicon
- `#[cfg(target_arch = "aarch64")]` NEON paths for F32, Q8_0, Q4_K, Q6_K
- F32: vfmaq_f32 4-wide accumulation
- Q8_0: vmull_s8 + vpadalq_s16 with scalar scale accumulation
- Q4_K: nibble extract + vmull with min/scale dequant
- Q6_K: 6-bit reconstruct (ql low4 + qh high2) + vmull
- Parity tests for all 4 kernel types

### 5.3 ‚Äî üöß Batch Prefill (Priority: MEDIUM)
- Batch Q/K/V projections: matmul instead of matvec for prefill
- Only for prefill phase; decode stays single-token

### 5.4 ‚Äî üöß Speculative Decoding (Priority: LOW)
- Evaluate after 5.1-5.3

---

## Phase 6: TEE Hardening ‚úÖ

**Goal**: Close security gaps for production TEE deployment.

### 6.1 ‚Äî ‚úÖ Timing Side-Channel Mitigation
- `timing_padding_ms` wired into both streaming and non-streaming chat paths
- ¬±20% jitter via existing `timing_padding()` method

### 6.2 ‚Äî ‚úÖ Memory Zeroization Audit
- `Drop` impl for `ForwardBuffers`: zeroizes all 12 Vec<f32> buffers
- `Drop` impl for `LayerKvCache`: zeroizes K/V f16 data
- `KvCache::clear()` now zeroizes data instead of just resetting length

### 6.3 ‚Äî ‚úÖ Startup Self-Test
- Embedded test vectors for rms_norm, vec_dot_f32, vec_dot_q8_0
- Runs at model load time, fails fast with clear error on mismatch
- Catches memory corruption in TEE before inference begins

---

## Phase 7: Ecosystem Integration ‚Äî Partially Complete

**Goal**: Make Power useful in the broader A3S platform.

### 7.1 ‚Äî ‚úÖ picolm Tool/Function Calling
- Wire `tool_parser::parse_tool_calls()` into picolm response stream
- Accumulate full generated text, parse tool calls on final chunk (EOS/stop/max_tokens)
- `has_tools` flag in GenerateParams, set from ChatRequest.tools
- Matches llamacpp/mistralrs backend pattern

### 7.2 ‚Äî üöß picolm Structured Output (JSON Schema) (Priority: MEDIUM)
- Implement grammar-constrained sampling in picolm
- Wire `json_schema.rs` grammar into the token sampling loop

### 7.3 ‚Äî ‚úÖ picolm Repeat/Frequency Penalty
- 64-token ring buffer tracking recent generated tokens
- `repeat_penalty` (multiplicative, llama.cpp style)
- `frequency_penalty` (proportional to count, OpenAI style)
- `presence_penalty` (flat if appeared, OpenAI style)
- Applied to logits before sampling

---

## What We Will NOT Build (First Principles Rejection)

### ‚ùå GPU Support for picolm
**Why not**: TEE memory (EPC) is CPU memory. GPU memory is outside the trust boundary. Adding GPU to picolm would undermine the entire security model. Use mistralrs/llamacpp for GPU inference outside TEE.

### ‚ùå Embeddings in picolm
**Why not**: Embedding models are small, don't need layer-streaming, and don't process sensitive user prompts (they process documents at indexing time, not query time). Use mistralrs for embeddings. Adding embedding support to picolm adds complexity without strengthening the privacy moat.

### ‚ùå Vision/Multimodal in picolm
**Why not**: Vision models require image encoders (ViT) that are architecturally different from text transformers. The complexity cost is high, and the TEE use case for vision is niche. Use mistralrs for vision.

### ‚ùå Model Quantization in picolm
**Why not**: Quantization is a one-time offline operation. It doesn't need to run inside TEE. Users quantize models before deployment.

### ‚ùå LoRA/Adapter Support in picolm
**Why not**: LoRA adds complexity to the forward pass and the supply-chain audit story. In TEE, you want to verify one specific model ‚Äî not a base + N adapters. If needed, merge LoRA into the base model before deployment.

---

## Completion Summary

```
Phase 4 (Production TEE Readiness):          ‚úÖ ALL COMPLETE
  4.5 Remove candle-core from picolm         ‚úÖ
  4.1 Multi-turn KV cache                    ‚úÖ
  4.2 Chat template from GGUF                ‚úÖ
  4.4 Stop sequence support                  ‚úÖ
  4.3 Configurable context length            ‚úÖ
  4.6 Integration tests                      ‚úÖ

Phase 6 (TEE Hardening):                     ‚úÖ ALL COMPLETE
  6.1 Timing side-channel mitigation         ‚úÖ
  6.2 Memory zeroization audit               ‚úÖ
  6.3 Startup self-test                      ‚úÖ

Phase 5 (Performance):                       üöß PARTIAL
  5.1 AVX2/AVX-512 vec_dot                   ‚úÖ (F32, Q8_0, Q4_K, Q6_K)
  5.2 NEON vec_dot                           ‚úÖ (F32, Q8_0, Q4_K, Q6_K)
  5.3 Batch prefill                          üöß (not started)
  5.4 Speculative decoding                   üöß (not started, low priority)

Phase 7 (Ecosystem):                         üöß PARTIAL
  7.3 Repeat penalty                         ‚úÖ
  7.1 Tool/function calling                  ‚úÖ
  7.2 Structured output                      üöß (not started)
```

**872 tests total** (unit + integration + real model).
**Recommended next**: 5.3 Batch prefill ‚Üí 7.2 Structured output.
